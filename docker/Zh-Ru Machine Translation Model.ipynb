{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mlbootcamp.ru/round/26/tasks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import pinyin.cedict\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.42.1'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jieba.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"河水不深不可以渡船来往的人必须涉水走过去。\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#He2shui3(реки вода) bu4shen1(не глубокая), bu4ke3yi3(нельзя, невозможно) \n",
    "# du4chuan2(переплыть напароме: «переправляться на другой берег + лодка»), \n",
    "# lai2wang3deren2(приходящие и уходящие люди; de–показатель определения) \n",
    "# bi4xu1(должны) she4shui3(переходить воду) zou3(пешком = в брод) \n",
    "# guo4qu(переходить: «миновать + уходить»; qu4–уходить)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package jieba:\n",
      "\n",
      "NAME\n",
      "    jieba\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    __main__\n",
      "    _compat\n",
      "    analyse (package)\n",
      "    finalseg (package)\n",
      "    lac_small (package)\n",
      "    posseg (package)\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Tokenizer\n",
      "    \n",
      "    class Tokenizer(builtins.object)\n",
      "     |  Tokenizer(dictionary=None)\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, dictionary=None)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  add_word(self, word, freq=None, tag=None)\n",
      "     |      Add a word to dictionary.\n",
      "     |      \n",
      "     |      freq and tag can be omitted, freq defaults to be a calculated value\n",
      "     |      that ensures the word can be cut out.\n",
      "     |  \n",
      "     |  calc(self, sentence, DAG, route)\n",
      "     |  \n",
      "     |  check_initialized(self)\n",
      "     |  \n",
      "     |  cut(self, sentence, cut_all=False, HMM=True, use_paddle=False)\n",
      "     |      The main function that segments an entire sentence that contains\n",
      "     |      Chinese characters into separated words.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: The str(unicode) to be segmented.\n",
      "     |          - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "     |          - HMM: Whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  cut_for_search(self, sentence, HMM=True)\n",
      "     |      Finer segmentation for search engines.\n",
      "     |  \n",
      "     |  del_word(self, word)\n",
      "     |      Convenient function for deleting a word.\n",
      "     |  \n",
      "     |  get_DAG(self, sentence)\n",
      "     |  \n",
      "     |  get_dict_file(self)\n",
      "     |  \n",
      "     |  initialize(self, dictionary=None)\n",
      "     |  \n",
      "     |  lcut(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  lcut_for_search(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  load_userdict(self, f)\n",
      "     |      Load personalized dict to improve detect rate.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - f : A plain text file contains words and their ocurrences.\n",
      "     |                Can be a file-like object, or the path of the dictionary file,\n",
      "     |                whose encoding must be utf-8.\n",
      "     |      \n",
      "     |      Structure of dict file:\n",
      "     |      word1 freq1 word_type1\n",
      "     |      word2 freq2 word_type2\n",
      "     |      ...\n",
      "     |      Word type may be ignored\n",
      "     |  \n",
      "     |  set_dictionary(self, dictionary_path)\n",
      "     |  \n",
      "     |  suggest_freq(self, segment, tune=False)\n",
      "     |      Suggest word frequency to force the characters in a word to be\n",
      "     |      joined or splitted.\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - segment : The segments that the word is expected to be cut into,\n",
      "     |                      If the word should be treated as a whole, use a str.\n",
      "     |          - tune : If True, tune the word frequency.\n",
      "     |      \n",
      "     |      Note that HMM may affect the final result. If the result doesn't change,\n",
      "     |      set HMM=False.\n",
      "     |  \n",
      "     |  tokenize(self, unicode_sentence, mode='default', HMM=True)\n",
      "     |      Tokenize a sentence and yields tuples of (word, start, end)\n",
      "     |      \n",
      "     |      Parameter:\n",
      "     |          - sentence: the str(unicode) to be segmented.\n",
      "     |          - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "     |          - HMM: whether to use the Hidden Markov Model.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  gen_pfdict(f)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    add_word(word, freq=None, tag=None) method of Tokenizer instance\n",
      "        Add a word to dictionary.\n",
      "        \n",
      "        freq and tag can be omitted, freq defaults to be a calculated value\n",
      "        that ensures the word can be cut out.\n",
      "    \n",
      "    calc(sentence, DAG, route) method of Tokenizer instance\n",
      "    \n",
      "    cut(sentence, cut_all=False, HMM=True, use_paddle=False) method of Tokenizer instance\n",
      "        The main function that segments an entire sentence that contains\n",
      "        Chinese characters into separated words.\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: The str(unicode) to be segmented.\n",
      "            - cut_all: Model type. True for full pattern, False for accurate pattern.\n",
      "            - HMM: Whether to use the Hidden Markov Model.\n",
      "    \n",
      "    cut_for_search(sentence, HMM=True) method of Tokenizer instance\n",
      "        Finer segmentation for search engines.\n",
      "    \n",
      "    del_word(word) method of Tokenizer instance\n",
      "        Convenient function for deleting a word.\n",
      "    \n",
      "    disable_parallel()\n",
      "    \n",
      "    enable_parallel(processnum=None)\n",
      "        Change the module's `cut` and `cut_for_search` functions to the\n",
      "        parallel version.\n",
      "        \n",
      "        Note that this only works using dt, custom Tokenizer\n",
      "        instances are not supported.\n",
      "    \n",
      "    get_DAG(sentence) method of Tokenizer instance\n",
      "    \n",
      "    get_FREQ lambda k, d=None\n",
      "    \n",
      "    get_dict_file() method of Tokenizer instance\n",
      "    \n",
      "    initialize(dictionary=None) method of Tokenizer instance\n",
      "    \n",
      "    lcut(*args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    lcut_for_search(*args, **kwargs) method of Tokenizer instance\n",
      "    \n",
      "    load_userdict(f) method of Tokenizer instance\n",
      "        Load personalized dict to improve detect rate.\n",
      "        \n",
      "        Parameter:\n",
      "            - f : A plain text file contains words and their ocurrences.\n",
      "                  Can be a file-like object, or the path of the dictionary file,\n",
      "                  whose encoding must be utf-8.\n",
      "        \n",
      "        Structure of dict file:\n",
      "        word1 freq1 word_type1\n",
      "        word2 freq2 word_type2\n",
      "        ...\n",
      "        Word type may be ignored\n",
      "    \n",
      "    log(...)\n",
      "        log(x, [base=math.e])\n",
      "        Return the logarithm of x to the given base.\n",
      "        \n",
      "        If the base not specified, returns the natural logarithm (base e) of x.\n",
      "    \n",
      "    md5 = openssl_md5(...)\n",
      "        Returns a md5 hash object; optionally initialized with a string\n",
      "    \n",
      "    setLogLevel(log_level)\n",
      "    \n",
      "    set_dictionary(dictionary_path) method of Tokenizer instance\n",
      "    \n",
      "    suggest_freq(segment, tune=False) method of Tokenizer instance\n",
      "        Suggest word frequency to force the characters in a word to be\n",
      "        joined or splitted.\n",
      "        \n",
      "        Parameter:\n",
      "            - segment : The segments that the word is expected to be cut into,\n",
      "                        If the word should be treated as a whole, use a str.\n",
      "            - tune : If True, tune the word frequency.\n",
      "        \n",
      "        Note that HMM may affect the final result. If the result doesn't change,\n",
      "        set HMM=False.\n",
      "    \n",
      "    tokenize(unicode_sentence, mode='default', HMM=True) method of Tokenizer instance\n",
      "        Tokenize a sentence and yields tuples of (word, start, end)\n",
      "        \n",
      "        Parameter:\n",
      "            - sentence: the str(unicode) to be segmented.\n",
      "            - mode: \"default\" or \"search\", \"search\" is for finer segmentation.\n",
      "            - HMM: whether to use the Hidden Markov Model.\n",
      "\n",
      "DATA\n",
      "    DEFAULT_DICT = None\n",
      "    DEFAULT_DICT_NAME = 'dict.txt'\n",
      "    DICT_WRITING = {}\n",
      "    PY2 = False\n",
      "    __license__ = 'MIT'\n",
      "    absolute_import = _Feature((2, 5, 0, 'alpha', 1), (3, 0, 0, 'alpha', 0...\n",
      "    check_paddle_install = {'is_paddle_installed': False}\n",
      "    default_encoding = 'utf-8'\n",
      "    default_logger = <Logger jieba (DEBUG)>\n",
      "    dt = <Tokenizer dictionary=None>\n",
      "    log_console = <StreamHandler stderr (NOTSET)>\n",
      "    pool = None\n",
      "    re_eng = re.compile('[a-zA-Z0-9]')\n",
      "    re_han_default = re.compile('([一-鿕a-zA-Z0-9+#&\\\\._%\\\\-]+)')\n",
      "    re_skip_default = re.compile('(\\r\\n|\\\\s)')\n",
      "    re_userdict = re.compile('^(.+?)( [0-9]+)?( [a-z]+)?$')\n",
      "    string_types = (<class 'str'>,)\n",
      "    unicode_literals = _Feature((2, 6, 0, 'alpha', 2), (3, 0, 0, 'alpha', ...\n",
      "    user_word_tag_tab = {}\n",
      "\n",
      "VERSION\n",
      "    0.42.1\n",
      "\n",
      "FILE\n",
      "    d:\\python37\\lib\\site-packages\\jieba\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(jieba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = jieba.lcut(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['河水', '不深', '不', '可以', '渡船', '来往', '的', '人', '必须', '涉水', '走', '过去', '。']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = []\n",
    "for word in words:\n",
    "    tr = pinyin.cedict.translate_word(word)\n",
    "    trans.append(tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['river water'],\n",
       " None,\n",
       " ['(negative prefix)', 'not', 'no'],\n",
       " ['can', 'may', 'possible', 'able to', 'not bad', 'pretty good'],\n",
       " ['ferry'],\n",
       " ['to come and go', 'to have dealings with', 'to be in relation with'],\n",
       " ['aim', 'clear'],\n",
       " ['man', 'person', 'people', 'CL:個|个[ge4],位[wei4]'],\n",
       " ['to have to', 'must', 'compulsory', 'necessarily'],\n",
       " None,\n",
       " ['to walk',\n",
       "  'to go',\n",
       "  'to run',\n",
       "  'to move (of vehicle)',\n",
       "  'to visit',\n",
       "  'to leave',\n",
       "  'to go away',\n",
       "  'to die (euph.)',\n",
       "  'from',\n",
       "  'through',\n",
       "  'away (in compound verbs, such as 撤走[che4 zou3])',\n",
       "  'to change (shape, form, meaning)'],\n",
       " ['(verb suffix)'],\n",
       " None]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bit6ddfec3d72d0445aa5d24384f848b5d9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
